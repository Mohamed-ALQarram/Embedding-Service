# ðŸ§  Embedding Service â€” BGE-M3 + FastAPI

A lightweight Python API built on **FastAPI** that generates **1024-D embeddings** using the **BAAI/BGE-M3** model.  
Perfect for **RAG systems, vector search, Qdrant, Pinecone, semantic similarity, document retrieval**, etc.

This README guides you from **zero â†’ running the API**, starting even from a clean Windows installation.

---

# ðŸš€ Features

- ðŸ”¥ FastAPI HTTP service
- ðŸ§  Uses BGE-M3 (state-of-the-art embedding model)
- ðŸ”¢ Outputs 1024-dim embedding vectors
- âš¡ CPU mode (default) â€” reliable on Windows
- ðŸ§± Clean, modular architecture
- ðŸ”„ Auto-download model from HuggingFace
- ðŸ“¦ Can also run fully offline (local model folder)
- ðŸ”’ Loads model only **once** in memory

---

# ðŸ“Œ 1. Install Python **3.10**

This project **requires Python 3.10**, because PyTorch + Transformers on Windows  
work best on this version.

Download Python 3.10.12:

ðŸ”— https://www.python.org/downloads/release/python-31012/

During installation:

âœ” Add Python to PATH  
âœ” Enable all optional features  
âœ” Install for all users (optional)

Confirm installation:

```bash
python --version


or:

py -3.10 --version


Output should be:

Python 3.10.x

ðŸ“Œ 2. Enable Windows Developer Mode

This prevents HuggingFace symlink issues and speeds up downloads.

Open RUN:

start ms-settings:developers


Enable:

âœ” Developer Mode

ðŸ“Œ 3. Clone the Repository
git clone https://github.com/Mohamed-ALQarram/Embedding-Service.git
cd Embedding-Service

ðŸ“Œ 4. Create Virtual Environment (venv)
If you have multiple Python versions (recommended):
py -3.10 -m venv venv

Otherwise:
python -m venv venv


Activate venv:

Windows
venv\Scripts\activate

Linux/Mac
source venv/bin/activate

ðŸ“Œ 5. Install Requirements
pip install -r requirements.txt

ðŸ“Œ 6. Configure the Model (Online Download)

Open config.py and set:

MODEL_NAME = "BAAI/bge-m3"
DEVICE = "cpu"


Why CPU?

Stable on all Windows machines

Avoids CUDA errors

Works out-of-the-box

âš  If you donâ€™t have CUDA â†’ DO NOT use "cuda".

ðŸ“Œ 7. Run the Service

Start FastAPI:

uvicorn main:app


âš  Important:
Do NOT use --reload because it will load the model twice.

You should see:

INFO: Uvicorn running on http://127.0.0.1:8000

ðŸ“Œ 8. Test the API (Swagger UI)

Open:

ðŸ‘‰ http://127.0.0.1:8000/docs

Try the /embed endpoint.

Example Input:
{
  "text": "hello world"
}

Example Output:
{
  "embedding": [ ...1024 float values... ]
}

ðŸ“Œ 9. Offline Mode (Local Model Folder)

If you want to run offline:

Create folder:

model/bge-m3/


Download these files from HuggingFace:

config.json

pytorch_model.bin

tokenizer.json

tokenizer_config.json

sentencepiece.bpe.model

special_tokens_map.json

Update config.py:

MODEL_NAME = "./model/bge-m3"
DEVICE = "cpu"

ðŸ“Œ 10. Project Structure
Embedding-Service/
â”‚
â”œâ”€â”€ main.py               # FastAPI routes
â”œâ”€â”€ model_loader.py       # Model + tokenizer initialization
â”œâ”€â”€ config.py             # Settings (model + device)
â”œâ”€â”€ requirements.txt
â””â”€â”€ model/ (optional)
    â””â”€â”€ bge-m3/

ðŸ“Œ 11. Example cURL Request
curl -X POST "http://127.0.0.1:8000/embed" \
     -H "Content-Type: application/json" \
     -d "{\"text\": \"Hello embedding world\"}"

ðŸ“Œ 12. Troubleshooting
âš  CUDA not available

Set:

DEVICE = "cpu"

âš  Model downloads twice

Donâ€™t use --reload.

âš  Tokenizer errors

Ensure you downloaded all tokenizer files if using offline mode.

âš  PyTorch errors

Ensure you're using Python 3.10 only.
